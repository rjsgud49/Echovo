import React, { useRef, useState } from 'react';

interface ISpeechRecognition extends EventTarget {
    lang: string;
    continuous: boolean;
    interimResults: boolean;
    onresult: ((this: ISpeechRecognition, ev: SpeechRecognitionEvent) => void) | null;
    start(): void;
    stop(): void;
    abort(): void;
}

type SpeechRecognitionConstructor = new () => ISpeechRecognition;

interface SpeechRecognitionEvent extends Event {
    results: SpeechRecognitionResultList;
}

declare global {
    interface Window {
        webkitSpeechRecognition: SpeechRecognitionConstructor;
        SpeechRecognition: SpeechRecognitionConstructor;
    }
}

interface ISpeechRecognition extends EventTarget {
    lang: string;
    continuous: boolean;
    interimResults: boolean;
    onresult: ((this: ISpeechRecognition, ev: SpeechRecognitionEvent) => void) | null;
    onerror: ((this: ISpeechRecognition, ev: Event) => void) | null;
    onstart: (() => void) | null;
    onend: (() => void) | null;
    start(): void;
    stop(): void;
    abort(): void;
}


const MicTestWithSTT: React.FC = () => {
    const [testing, setTesting] = useState(false);
    const [volume, setVolume] = useState(0);
    const [transcript, setTranscript] = useState('');
    const [transcriptVisible, setTranscriptVisible] = useState(false);

    const recognitionRef = useRef<ISpeechRecognition | null>(null);
    const audioContextRef = useRef<AudioContext | null>(null);
    const animationRef = useRef<number | null>(null);

    const startTest = async () => {
        console.log('ğŸ¬ [1] startTest() í˜¸ì¶œë¨');

        try {
            console.log('ğŸ›ï¸ [2] getUserMedia ìš”ì²­ ì‹œì‘...');
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            console.log('âœ… [3] ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ íšë“ ì„±ê³µ:', stream);

            const tracks = stream.getAudioTracks();
            if (tracks.length === 0) {
                console.warn('âš ï¸ [3-1] ë§ˆì´í¬ íŠ¸ë™ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ');
            } else {
                console.log('ğŸ” [3-2] ë§ˆì´í¬ íŠ¸ë™ ì •ë³´:', tracks[0]);
            }

            const audioCtx = new AudioContext();
            console.log('ğŸ§ [4] AudioContext ìƒì„± ì™„ë£Œ');

            audioContextRef.current = audioCtx;
            const source = audioCtx.createMediaStreamSource(stream);
            const analyser = audioCtx.createAnalyser();
            analyser.fftSize = 64;
            const dataArray = new Uint8Array(analyser.frequencyBinCount);
            source.connect(analyser);
            console.log('ğŸ”— [5] Audio íŒŒì´í”„ ì—°ê²° ì™„ë£Œ');

            const animate = () => {
                analyser.getByteFrequencyData(dataArray);
                const avg = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;
                setVolume(avg);
                animationRef.current = requestAnimationFrame(animate);
            };
            animate();
            console.log('ğŸ“Š [6] ì• ë‹ˆë©”ì´ì…˜ ì‹œì‘ë¨');

            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!SpeechRecognition) {
                alert('ì´ ë¸Œë¼ìš°ì €ëŠ” SpeechRecognitionì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
                console.error('âŒ [7] SpeechRecognition ìƒì„± ë¶ˆê°€');
                return;
            }
            console.log('âœ… [7] SpeechRecognition ìƒì„±ë¨');

            const recognition = new SpeechRecognition();
            recognition.lang = 'ko-KR';
            recognition.continuous = true;
            recognition.interimResults = true;

            recognition.onresult = (event: SpeechRecognitionEvent) => {
                const result = Array.from(event.results)
                    .map((r) => r[0].transcript)
                    .join('');
                console.log('ğŸ—£ï¸ [8] ìŒì„± ì¸ì‹ ê²°ê³¼:', result);
                setTranscript(result);
            };

            recognition.onerror = (e) => {
                console.error('âŒ [9] SpeechRecognition ì—ëŸ¬ ë°œìƒ:', e);
            };

            recognition.onstart = () => {
                console.log('ğŸ¤ [10] SpeechRecognition ì‹œì‘ë¨');
            };

            recognition.onend = () => {
                console.log('ğŸ›‘ [11] SpeechRecognition ì¢…ë£Œë¨');
            };

            console.log('ğŸš¦ [12] SpeechRecognition.start() í˜¸ì¶œ');
            recognition.start();
            recognitionRef.current = recognition;
            console.log('ğŸŸ¢ [13] SpeechRecognition ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘ ìš”ì²­ ì™„ë£Œ');

            setTranscriptVisible(true);
            setTesting(true);
            console.log('ğŸ [14] ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ: testing = true');
        } catch (err) {
            console.error('âŒ [E] ë§ˆì´í¬ ê¶Œí•œ ì—ëŸ¬ ë˜ëŠ” AudioContext ì—ëŸ¬:', err);
            alert('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
        }
    };


    const stopTest = () => {
        console.log('â›” stopTest() í˜¸ì¶œë¨');

        if (animationRef.current) {
            cancelAnimationFrame(animationRef.current);
            animationRef.current = null;
            console.log('ğŸ›‘ ì• ë‹ˆë©”ì´ì…˜ ì¤‘ë‹¨ë¨');
        }

        if (audioContextRef.current) {
            audioContextRef.current.close();
            audioContextRef.current = null;
            console.log('ğŸ§ AudioContext ì¢…ë£Œë¨');
        }

        if (recognitionRef.current) {
            recognitionRef.current.stop();
            console.log('ğŸ›‘ SpeechRecognition ì¤‘ë‹¨ë¨');
        }
        recognitionRef.current = null;

        setTesting(false);
        setVolume(0);
        setTranscript('');
        setTranscriptVisible(false);
        console.log('ğŸ” ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ');
    };

    return (
        <div className="flex flex-col items-center bg-white rounded shadow p-6 w-full">
            <p className="text-sm text-gray-600 mb-4">
                ë§ˆì´í¬ì— ëŒ€ê³  ì•„ë¬´ ë§ì´ë‚˜ í•´ë³´ì„¸ìš”. í…ìŠ¤íŠ¸ë¡œ ì¸ì‹ë©ë‹ˆë‹¤!
            </p>

            <button
                onClick={testing ? stopTest : startTest}
                className={`mb-6 px-6 py-2 rounded text-white font-semibold shadow ${testing
                    ? 'bg-red-500 hover:bg-red-600'
                    : 'bg-green-500 hover:bg-green-600'
                    }`}
            >
                {testing ? 'í…ŒìŠ¤íŠ¸ ì •ì§€í•˜ê¸°' : 'í™•ì¸í•˜ê¸°'}
            </button>

            <div className="flex items-end h-24 gap-[2px] mb-6">
                {Array.from({ length: 32 }).map((_, i) => (
                    <div
                        key={i}
                        className="w-[4px] transition-all duration-100"
                        style={{
                            height: `${Math.max(10, volume - i * 2)}px`,
                            backgroundColor:
                                volume > i * 2
                                    ? `hsl(${50 + i * 3}, 100%, 60%)`
                                    : '#ccc',
                        }}
                    />
                ))}
            </div>

            {testing && (
                <p className="text-sm text-blue-600 mb-2">
                    ğŸ§ ìŒì„±ì„ ë“£ê³  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘ì…ë‹ˆë‹¤...
                </p>
            )}

            {transcriptVisible && transcript && (
                <div className="w-full max-w-lg bg-gray-100 rounded p-4 text-left text-sm text-gray-800">
                    <p className="font-semibold mb-2">ğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸:</p>
                    <div className="whitespace-pre-wrap">{transcript}</div>
                </div>
            )}
        </div>
    );
};

export default MicTestWithSTT;
