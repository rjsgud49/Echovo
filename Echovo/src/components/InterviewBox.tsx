import React, { useEffect, useRef, useState } from 'react';
import Recorder from './Recorder';
import type { RecorderHandle } from './Recorder';
import { generateQuestion, getScoredFeedback, transcribeAudio, summarizeQuestion } from '../utils/openai';
import type { RecordItem, Props } from '../types/interview';

const InterviewBox: React.FC<Props> = ({ field: propField, stack: propStack, onLogUpdated }) => {
    const [question, setQuestion] = useState('');
    const [answer, setAnswer] = useState('');
    const [feedback, setFeedback] = useState('');
    const [modelAnswer, setModelAnswer] = useState('');
    const [liveTranscript, setLiveTranscript] = useState('');
    const [recording, setRecording] = useState(false);
    const [seconds, setSeconds] = useState(0);
    const [isSaving, setIsSaving] = useState(false);
    const [isDuplicate, setIsDuplicate] = useState(false);
    const [questionReady, setQuestionReady] = useState(false);

    const recorderRef = useRef<RecorderHandle>(null);
    const recognitionRef = useRef<SpeechRecognition | null>(null);
    const startTimeRef = useRef<number | null>(null);
    const recordingDurationRef = useRef<number>(0);

    const [field, setField] = useState('');
    const [stack, setStack] = useState('');

    useEffect(() => {
        const setup = async () => {
            let f = propField;
            let s = propStack;
            if (!f || !s) {
                const saved = localStorage.getItem('interviewUserInfo');
                if (saved) {
                    const parsed = JSON.parse(saved);
                    f = parsed.field;
                    s = parsed.stack;
                }
            }
            setField(f ?? '');
            setStack(s ?? '');

            if (f && s) {
                const q = await generateQuestion(f, s);
                setQuestion(q);
                setQuestionReady(true);
            }
        };
        setup();
    }, [propField, propStack]);

    useEffect(() => {
        let timer: ReturnType<typeof setInterval>;
        if (recording) {
            timer = setInterval(() => setSeconds((prev) => prev + 1), 1000);
        } else {
            setSeconds(0);
        }
        return () => clearInterval(timer);
    }, [recording]);

    const startSpeechRecognition = () => {
        const SpeechRecognitionConstructor =
            window.SpeechRecognition || window.webkitSpeechRecognition;

        if (!SpeechRecognitionConstructor) {
            alert('ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
            return;
        }

        const recognition = new (SpeechRecognitionConstructor as unknown as new () => SpeechRecognition)();
        recognition.lang = 'ko-KR';
        recognition.interimResults = true;

        recognition.onresult = (event: SpeechRecognitionEvent) => {
            let text = '';
            for (let i = event.resultIndex; i < event.results.length; ++i) {
                if (!event.results[i].isFinal) {
                    text += event.results[i][0].transcript;
                }
            }
            console.log('ğŸ¤ ì‹¤ì‹œê°„ ì¸ì‹:', text);
            setLiveTranscript(text);
        };

        recognition.onend = () => recognition.start();

        recognitionRef.current = recognition;
        recognition.start();
    };

    const stopSpeechRecognition = () => {
        recognitionRef.current?.stop();
        recognitionRef.current = null;
    };

    const startRecording = () => {
        console.log('â–¶ï¸ ë…¹ìŒ ì‹œì‘');
        startTimeRef.current = Date.now();
        setRecording(true);
        setAnswer('');
        setFeedback('');
        setModelAnswer('');
        setLiveTranscript('');
        startSpeechRecognition();
        recorderRef.current?.start();
    };

    const stopRecording = () => {
        console.log('â¹ï¸ ë…¹ìŒ ì¤‘ì§€');
        recorderRef.current?.stop();
        stopSpeechRecognition();
        setRecording(false);
        if (startTimeRef.current) {
            recordingDurationRef.current = Math.floor((Date.now() - startTimeRef.current) / 1000);
        }
    };

    const handleRecordingStop = async (blob: Blob) => {
        console.log('ğŸ“¦ handleRecordingStop triggered');
        setIsSaving(true);
        const transcript = await transcribeAudio(blob);
        const summary = await summarizeQuestion(question);
        const { feedback: fb, score, modelAnswer: example } = await getScoredFeedback(question, transcript);

        if (!transcript?.trim() || !fb?.trim()) {
            setIsSaving(false);
            return;
        }

        const prev = JSON.parse(localStorage.getItem('interviewLogs') || '[]') as RecordItem[];
        const alreadyExists = prev.some((log) => log.question === question);
        if (alreadyExists) {
            setIsDuplicate(true);
            setIsSaving(false);
            return;
        }

        const record: RecordItem = {
            id: Date.now().toString(),
            date: new Date().toLocaleString('ko-KR', { hour12: false }),
            question,
            summary,
            answer: transcript,
            feedback: fb,
            score,
            duration: recordingDurationRef.current,
        };

        try {
            localStorage.setItem('interviewLogs', JSON.stringify([record, ...prev]));
            onLogUpdated?.();
        } catch (e) {
            console.error('âŒ ì €ì¥ ì‹¤íŒ¨:', e);
        }

        setAnswer(transcript);
        setFeedback(fb);
        setModelAnswer(example);
        setLiveTranscript('');
        setIsSaving(false);
        setIsDuplicate(true);
    };

    const regenerateQuestion = async () => {
        setQuestionReady(false);
        setIsDuplicate(false);
        setAnswer('');
        setFeedback('');
        setModelAnswer('');
        setLiveTranscript('');
        const q = await generateQuestion(field, stack);
        setQuestion(q);
        setQuestionReady(true);
    };

    return (
        <div className="flex flex-col items-center justify-start min-h-screen p-8 bg-gray-50 text-center space-y-8">
            <div className="max-w-3xl">
                <p className="text-lg font-semibold text-gray-900">
                    ğŸ¤ <strong>ì§ˆë¬¸:</strong> {question}
                </p>
            </div>

            <button
                className={`w-[150px] h-[150px] rounded-full text-white text-xl font-bold shadow transition ${recording ? 'bg-red-500 hover:bg-red-600' : 'bg-green-500 hover:bg-green-600'}`}
                onClick={recording ? stopRecording : startRecording}
                disabled={!questionReady || isSaving}
            >
                {recording ? 'ì¤‘ì§€í•˜ê¸°' : 'ì‹œì‘í•˜ê¸°'}
            </button>

            {recording && (
                <div className="text-gray-700 space-y-2">
                    <p>â±ï¸ ê²½ê³¼ ì‹œê°„: <strong>{seconds}ì´ˆ</strong></p>
                    {liveTranscript && (
                        <p className="text-sm text-gray-600">ğŸ¤ <strong>ì‹¤ì‹œê°„:</strong> {liveTranscript}</p>
                    )}
                </div>
            )}

            {!recording && isDuplicate && (
                <button
                    onClick={regenerateQuestion}
                    className="mt-2 px-6 py-3 bg-blue-500 hover:bg-blue-600 text-white font-semibold rounded shadow"
                >
                    ğŸ”„ ë‹¤ìŒ ì§ˆë¬¸ ìš”ì²­
                </button>
            )}

            <div className="text-left max-w-3xl space-y-4 mt-8">
                {answer && <p><strong>ğŸ“ ë‚´ ë‹µë³€:</strong> {answer}</p>}
                {feedback && <p><strong>ğŸ’¡ í”¼ë“œë°±:</strong> {feedback}</p>}
                {modelAnswer && <p><strong>ğŸ“˜ ëª¨ë²”ë‹µì•ˆ:</strong> {modelAnswer}</p>}
            </div>

            <Recorder ref={recorderRef} onStop={handleRecordingStop} maxDuration={60} />
        </div>
    );
};

export default InterviewBox;
